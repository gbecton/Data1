{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Explore/Clean.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtlYTY-W_Or4"
      },
      "source": [
        "# Data Exploring & Cleaning\n",
        "##  Gabriel Becton\n",
        "Today we will focus on taking a first look at some data sets and cleaning the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrketcW5_Or7"
      },
      "source": [
        "## Cleaning and Munging a Simple Data Frame\n",
        "\n",
        "Before working with a large data set, let us first practice with a small amount of data in a simple data frame.  This example comes from [here](https://github.com/ajcr/100-pandas-puzzles/blob/master/100-pandas-puzzles-with-solutions.ipynb). The data consists of some made-up flight information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "787O1YpT_Or8"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'From_To': ['LoNDon_paris', 'MAdrid_miLAN', 'londON_StockhOlm', \n",
        "                               'Budapest_PaRis', 'Brussels_londOn'],\n",
        "              'FlightNumber': [10045, np.nan, 10065, np.nan, 10085],\n",
        "              'RecentDelays': [[23, 47], [], [24, 43, 87], [13], [67, 32]],\n",
        "                   'Airline': ['KLM(!)', '<Air France> (12)', '(British Airways. )', \n",
        "                               '12. Air France', '\"Swiss Air\"']})\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "top0TKky_Or_"
      },
      "source": [
        "Some values in the the FlightNumber column are missing. These numbers are meant to increase by 10 with each row so 10055 and 10075 need to be put in place. Fill in these missing numbers and make the column an integer column (instead of a float column). The pandas `interpolate` function fills in NaNs with interpolated values and is described [here](http://pandas.pydata.org/pandas-docs/version/0.16.2/generated/pandas.DataFrame.interpolate.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njgjrurV_Or_"
      },
      "source": [
        "df['FlightNumber'] = df['FlightNumber'].interpolate().astype(int)\n",
        "df['FlightNumber']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0qbHbPS_OsB"
      },
      "source": [
        "The From_To column would be better as two separate columns! Split each string on the underscore delimiter _ to give a new temporary DataFrame with the correct values. Assign the correct column names to this temporary DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15qfuTwe_OsB"
      },
      "source": [
        "temp = df.From_To.str.split('_', expand=True)\n",
        "temp.columns = ['From', 'To']\n",
        "temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSzMmzAq_OsD"
      },
      "source": [
        "Notice how the capitalisation of the city names is all mixed up in this temporary DataFrame. Standardise the strings so that only the first letter is uppercase (e.g. \"LoNDon\" should become \"London\".)  The string method `capitalize()` does just that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRaRrtPf_OsE"
      },
      "source": [
        "temp['From'] = temp['From'].str.capitalize()\n",
        "temp['To'] = temp['To'].str.capitalize()\n",
        "temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2QPnQjU_OsF"
      },
      "source": [
        "Delete the From_To column from df and attach the temporary DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edNn0cX9_OsG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6ceab9b8-c8d8-4d44-d7c9-246a7d7599eb"
      },
      "source": [
        "df = df.drop('From_To', axis=1)\n",
        "df = df.join(temp)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FlightNumber</th>\n",
              "      <th>RecentDelays</th>\n",
              "      <th>Airline</th>\n",
              "      <th>From</th>\n",
              "      <th>To</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10045</td>\n",
              "      <td>[23, 47]</td>\n",
              "      <td>KLM(!)</td>\n",
              "      <td>London</td>\n",
              "      <td>Paris</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10055</td>\n",
              "      <td>[]</td>\n",
              "      <td>&lt;Air France&gt; (12)</td>\n",
              "      <td>Madrid</td>\n",
              "      <td>Milan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10065</td>\n",
              "      <td>[24, 43, 87]</td>\n",
              "      <td>(British Airways. )</td>\n",
              "      <td>London</td>\n",
              "      <td>Stockholm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10075</td>\n",
              "      <td>[13]</td>\n",
              "      <td>12. Air France</td>\n",
              "      <td>Budapest</td>\n",
              "      <td>Paris</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10085</td>\n",
              "      <td>[67, 32]</td>\n",
              "      <td>\"Swiss Air\"</td>\n",
              "      <td>Brussels</td>\n",
              "      <td>London</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   FlightNumber  RecentDelays              Airline      From         To\n",
              "0         10045      [23, 47]               KLM(!)    London      Paris\n",
              "1         10055            []    <Air France> (12)    Madrid      Milan\n",
              "2         10065  [24, 43, 87]  (British Airways. )    London  Stockholm\n",
              "3         10075          [13]       12. Air France  Budapest      Paris\n",
              "4         10085      [67, 32]          \"Swiss Air\"  Brussels     London"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9gUuJfV_OsH"
      },
      "source": [
        "In the Airline column, you can see some extra puctuation and symbols have appeared around the airline names. Pull out just the airline name. E.g. '(British Airways. )' should become 'British Airways'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlcV29DH_OsI"
      },
      "source": [
        "df['Airline'] = df['Airline'].str.extract('([a-zA-Z\\s]+)', expand=False).str.strip()\n",
        "# note: using .strip() gets rid of any leading/trailing spaces\n",
        "df.Airline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFgZD4Dd_OsJ"
      },
      "source": [
        "In the RecentDelays column, the values have been entered into the DataFrame as a list. We would like each first value in its own column, each second value in its own column, and so on. If there isn't an Nth value, the value should be NaN.\n",
        "\n",
        "Expand the Series of lists into a DataFrame named delays, rename the columns delay_1, delay_2, etc. and replace the unwanted RecentDelays column in df with delays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhWpyEhI_OsK"
      },
      "source": [
        "# there are several ways to do this, but the following approach is one of the simplest\n",
        "\n",
        "delays = df['RecentDelays'].apply(pd.Series)\n",
        "\n",
        "delays.columns = ['delay_{}'.format(n) for n in range(1, len(delays.columns)+1)]\n",
        "\n",
        "df = df.drop('RecentDelays', axis=1).join(delays)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiXy2XMr_OsL"
      },
      "source": [
        "The DataFrame should look much better now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfmlmG26_OsM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8f9b631b-3f93-4641-8e5e-4da0d5be79c8"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FlightNumber</th>\n",
              "      <th>Airline</th>\n",
              "      <th>From</th>\n",
              "      <th>To</th>\n",
              "      <th>delay_1</th>\n",
              "      <th>delay_2</th>\n",
              "      <th>delay_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10045</td>\n",
              "      <td>KLM</td>\n",
              "      <td>London</td>\n",
              "      <td>Paris</td>\n",
              "      <td>23.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10055</td>\n",
              "      <td>Air France</td>\n",
              "      <td>Madrid</td>\n",
              "      <td>Milan</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10065</td>\n",
              "      <td>British Airways</td>\n",
              "      <td>London</td>\n",
              "      <td>Stockholm</td>\n",
              "      <td>24.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>87.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10075</td>\n",
              "      <td>Air France</td>\n",
              "      <td>Budapest</td>\n",
              "      <td>Paris</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10085</td>\n",
              "      <td>Swiss Air</td>\n",
              "      <td>Brussels</td>\n",
              "      <td>London</td>\n",
              "      <td>67.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   FlightNumber          Airline      From  ... delay_1  delay_2  delay_3\n",
              "0         10045              KLM    London  ...    23.0     47.0      NaN\n",
              "1         10055       Air France    Madrid  ...     NaN      NaN      NaN\n",
              "2         10065  British Airways    London  ...    24.0     43.0     87.0\n",
              "3         10075       Air France  Budapest  ...    13.0      NaN      NaN\n",
              "4         10085        Swiss Air  Brussels  ...    67.0     32.0      NaN\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29GAawRb_OsN"
      },
      "source": [
        "Finally, let's replace the NaNs in the delay columns with zeros.  Take a look at the pandas [fillna](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html) method for a suggestion on how to do this. Also display the modified DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx9pId-g_OsO"
      },
      "source": [
        "# Task 1: Enter code in this cell to change the NaN delays to zeros.  Re-display the modified DataFrame.\n",
        "df['delay_1'] = df['delay_1'].fillna(value=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CckKr_oBrDfZ"
      },
      "source": [
        "df['delay_1']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo7laMj8YM5l"
      },
      "source": [
        "df['delay_2'] = df['delay_2'].fillna(value=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kqP5fHlrIrV"
      },
      "source": [
        "df['delay_2']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRyfJTV2YONY"
      },
      "source": [
        "df['delay_3'] = df['delay_3'].fillna(value=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX0NzbkWrKGr"
      },
      "source": [
        "df['delay_3']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg9hf8B0_OsP"
      },
      "source": [
        "## Building Permit Data \n",
        "\n",
        "Next we will practice cleaning a larger dataset from Kaggle.  This dataset is described in detail [here](https://www.kaggle.com/aparnashastry/building-permit-applications-data).  Go to that page to look at the column details as well as some summary statistics and histograms. The analysis we will perform will follow closely the one from [here](https://www.kaggle.com/chrisbow/cleaning-data-with-python-challenge-day-1).\n",
        "\n",
        "The dataset describes building permits issued for San Francisco from Jan 1, 2013 to Feb 25th 2018.  First, we will download the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPoH1QR5B4lk"
      },
      "source": [
        "# Replace 'username' with your username from your kaggle.json file\n",
        "# Replace 'yourkey' with your key from your kaggle.json file\n",
        "# This key will be a long string of numbers and letters\n",
        "\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME']='gabrielbecton'\n",
        "os.environ['KAGGLE_KEY']='62a34da3f59850bcad7cf043f0d2a4cd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJQQqOhR_OsQ"
      },
      "source": [
        "! kaggle datasets download -d aparnashastry/building-permit-applications-data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTUejkTI_OsR"
      },
      "source": [
        "Next, unzip the file and read the contents into a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWJlSJLp_OsS"
      },
      "source": [
        "! unzip building-permit-applications-data.zip\n",
        "\n",
        "sfPermits = pd.read_csv(\"Building_Permits.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv0YQ8ZL_OsU"
      },
      "source": [
        "Next, let us take a first look at the data.  Display some randomly selected rows from our data.  We will first set the random seed so that we get the same rows picked if we re-run the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XlEEMEI_OsU"
      },
      "source": [
        "np.random.seed(0)\n",
        "sfPermits.sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgVDp1oZ_OsW"
      },
      "source": [
        "Quite a few missing values visible already, and we've only looked at five rows of the dataset, cleaning will be required...\n",
        "\n",
        "### Find out what percent of the sf_permit dataset is missing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA5OHpkr_OsW"
      },
      "source": [
        "# Calculate total number of cells in dataframe\n",
        "totalCells = np.product(sfPermits.shape)\n",
        "\n",
        "# Count number of missing values per column\n",
        "missingCount = sfPermits.isnull().sum()\n",
        "\n",
        "# Calculate total number of missing values\n",
        "totalMissing = missingCount.sum()\n",
        "\n",
        "# Calculate percentage of missing values\n",
        "print(\"The SF Permits dataset contains\", round(((totalMissing/totalCells) * 100), 2), \"%\", \"missing values.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2FnZMNk_OsY"
      },
      "source": [
        "Look at the columns Street Number Suffix and Zipcode from the sf_permits datasets. Both of these contain missing values. Which, if either, of these are missing because they don't exist? Which, if either, are missing because they weren't recorded?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMOiA5nf_OsY"
      },
      "source": [
        "missingCount[['Street Number Suffix', 'Zipcode']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac_pyxWp_Osa"
      },
      "source": [
        "Looks like a lot more missing values for street number suffix than zipcode. Let's check out the percentages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLP7zRwi_Osa"
      },
      "source": [
        "print(\"Percent missing data in Street Number Suffix column =\", (round(((missingCount['Street Number Suffix'] / sfPermits.shape[0]) * 100), 2)))\n",
        "print(\"Percent missing data in Zipcode column =\", (round(((missingCount['Zipcode'] / sfPermits.shape[0]) * 100), 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tthKtNR_Osd"
      },
      "source": [
        "As every address has a Zipcode, it looks like the missing values for this column are due to the values not being recorded. For the Street Number Suffix column, it is likely very few properties will have a suffix to the number, I see a lot of 3s, 18s, 46s, but not nearly as many 36A or 18B, so it is likely that these are missing as they don't exist.\n",
        "\n",
        "### Try removing all the rows from the sf_permits dataset that contain missing values. How many are left?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqnfs6P2_Ose"
      },
      "source": [
        "sfPermits.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfxOgTyO_Osg"
      },
      "source": [
        "If we drop all rows that contain a missing value, we greatly simplify our dataset. So simple, we can go for an early lunch. Every row contains at least one missing value (well, we know from our Street Number Suffix answer above that simply eliminating those gets rid of nearly 99% of our data), so we end up with a dataframe of column headers.\n",
        "\n",
        "### Now try removing all the columns with empty values. Now how much of your data is left?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgtNI1Sb_Osh"
      },
      "source": [
        "sfPermitsCleanCols = sfPermits.dropna(axis=1)\n",
        "sfPermitsCleanCols.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSV8cZQF_Osi"
      },
      "source": [
        "print(\"Columns in original dataset: %d \\n\" % sfPermits.shape[1])\n",
        "print(\"Columns with na's dropped: %d\" % sfPermitsCleanCols.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS9f-h47_Osj"
      },
      "source": [
        "Well, that gives us a clean set of values, but we've sacrificed a lot of variables in the process...\n",
        "\n",
        "Try replacing all the NaN's in the sf_permit data with data from the row that comes directly before it and then replace all the remaining na's with 0. Since the building permits in each row are likely unrelated to the ones before and after, this is not an awesome technique but is a simple way to fill in missing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYY0_K43_Osk"
      },
      "source": [
        "imputeSfPermits = sfPermits.fillna(method='ffill', axis=0).fillna(\"0\")\n",
        "\n",
        "imputeSfPermits.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSYhEKiU_Osl"
      },
      "source": [
        "### Calculate the average GPS coordinates for San Francisco building permits\n",
        "\n",
        "The Location column holds two values per building permit that look like (37.785719256680785, -122.40852313194863).  The first number is the GPS latitude and the second the GPS longitude.  Replace the Location column with two columns, one named GPS Lat and the other names GPS Lon that separately hold the latitude and longitude data. There are multiple methods for doing this, one uses `.str.split`, `.str.replace`, and `.astype('float')`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPE5Ci_y_Osm"
      },
      "source": [
        "# Enter code in this cell to replace the Location column of imputeSFPermits with two columns\n",
        "# that hold the GPS latitude and longitude, respectively.\n",
        "\n",
        "temp1 = sfPermits.Location.str.split(',', expand=True)\n",
        "temp1.columns = ['GPS Lat', 'GPS Lon']\n",
        "temp1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jowjhN3L_Osn"
      },
      "source": [
        "Finally, calculate the average latitude and average longitude of the San Francisco Permit locations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O3S1MCd_Osn"
      },
      "source": [
        "# Enter code to compute the average lat and lon values.\n",
        "temp1.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCcXioH5mD5B"
      },
      "source": [
        "temp1.dropna()\n",
        "temp2 = temp1.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcmSCrdomZKi"
      },
      "source": [
        "temp2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln_t2PUCnJWc"
      },
      "source": [
        "import statistics\n",
        "temp3 = temp2.astype(float)\n",
        "\n",
        "statistics.mean(temp3['GPS Lon'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVC9ZvxgsZiH"
      },
      "source": [
        "statistics.mean(temp3['GPS Lat'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg1fnWia_Osp"
      },
      "source": [
        "## IMDB Movie Data\n",
        "\n",
        "Next we will be inspecting and cleaning a dataset from Kaggle that consists of data scraped from the IMDB website.  The Kaggle site for the data is [here](https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset) and we will be following an analysis recommended by [this site](http://www.developintelligence.com/blog/2017/08/data-cleaning-pandas-python/).\n",
        "\n",
        "First, we will download and unzip the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bURjO97l_Osp"
      },
      "source": [
        "! kaggle datasets download -d carolzhangdc/imdb-5000-movie-dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTxunezn_Osq"
      },
      "source": [
        "! unzip imdb-5000-movie-dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ejVExQp_Oss"
      },
      "source": [
        "Then read the data into a DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4plz788p_Ost"
      },
      "source": [
        "imdb_df = pd.read_csv('movie_metadata.csv')\n",
        "imdb_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCc1ANP3_Osu"
      },
      "source": [
        "We can see that there are some missing values and that some movies have very incomplete information (look at row 4, for example).  We can eliminate these rows from our set. With the `dropna` method used below, axis = 0 means go by row (axis = 1 means go by column) and thresh = 20 means drop any rows with fewer than 20 non-NaN values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "227eBQbX_Osv"
      },
      "source": [
        "numrows_before, _ = imdb_df.shape\n",
        "imdb_df = imdb_df.dropna(axis=0, thresh=20) \n",
        "numrows_after, _ = imdb_df.shape\n",
        "numrows_removed = numrows_before - numrows_after\n",
        "\n",
        "print('{} rows removed'.format(numrows_removed))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDcTRdXn_Osw"
      },
      "source": [
        "imdb_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmoLguyA_Osy"
      },
      "source": [
        "Of course, we could also drop rows with any missing information via `.dropna()` but we would likely lose a lot of our dataset.  If we want to drop rows that contain only missing information values, we can use `.dropna(how='all')` but it is unlikely that we would have rows with absolutely no information.\n",
        "\n",
        "Let us next investigate which are the most problematic columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xDqlZZ1_Osy"
      },
      "source": [
        "missingCount = imdb_df.isnull().sum()\n",
        "missingCount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVoVHYIO_Osz"
      },
      "source": [
        "Some columns contain strings.  It would be appropriate to replace missing string values with an empty string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWtrTo4d_Os0"
      },
      "source": [
        "imdb_df['color'] = imdb_df['color'].fillna('')\n",
        "missingCount = imdb_df.isnull().sum()\n",
        "missingCount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHGqNrkL_Os2"
      },
      "source": [
        "# Enter code to replace the rest of the missing string values with empty strings\n",
        "imdb_df['director_name'] = imdb_df['director_name'].fillna('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht1YSg6xwPza"
      },
      "source": [
        "imdb_df['actor_2_name'] = imdb_df['actor_2_name'].fillna('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKBz7lH3wWe9"
      },
      "source": [
        "imdb_df['actor_1_name'] = imdb_df['actor_1_name'].fillna('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYVtTFExwaSx"
      },
      "source": [
        "imdb_df['actor_3_name'] = imdb_df['actor_3_name'].fillna('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z15L6G2wd6a"
      },
      "source": [
        "imdb_df['language'] = imdb_df['language'].fillna('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dds-wdrgwmIv"
      },
      "source": [
        "imdb_df['country'] = imdb_df['country'].fillna('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yky2A-e7wqry"
      },
      "source": [
        "imdb_df['plot_keywords'] = imdb_df['plot_keywords'].fillna('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DptWt7how-UE"
      },
      "source": [
        "imdb_df['content_rating'] = imdb_df['content_rating'].fillna('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OUidECO_Os3"
      },
      "source": [
        "Let's say that the year that the movie came out is very important to us and we just don't care about movies where that information is not available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqtlDucQ_Os3"
      },
      "source": [
        "imdb_df = imdb_df.dropna(subset=['title_year'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_mMfu26_Os5"
      },
      "source": [
        "`title_year` may be a confusing column header, so we can rename it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iG_RhGKf_Os5"
      },
      "source": [
        "imdb_df = imdb_df.rename(columns = 5555{'title_year':'release_date'})\n",
        "imdb_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4PKQswb_Os6"
      },
      "source": [
        "Let's make a histogram of release dates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22O-cpqT_Os7"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "imdb_df.hist(column='release_date', bins=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdF4rjJ8_Os8"
      },
      "source": [
        "# Calculate and output the min, max, mean, and standard deviation of the imdb_score.\n",
        "# Also plot a histogram of the values stored in this column.\n",
        "import statistics\n",
        "import numpy as np\n",
        "\n",
        "imdb_df_mean = statistics.mean(imdb_df['imdb_score'])\n",
        "imdb_df_min = min(imdb_df['imdb_score'])\n",
        "imdb_df_max = max(imdb_df['imdb_score'])\n",
        "imdb_df_sd = statistics.stdev(imdb_df['imdb_score'])\n",
        "\n",
        "imdb_df.hist(column='imdb_score')\n",
        "\n",
        "print('the mean of imdb_score is ', imdb_df_mean)\n",
        "print('the max value of imdb_score is ',imdb_df_max)\n",
        "print('the min value of imdb_score is ',imdb_df_min)\n",
        "print('the standard deviation of imdb_score is ',imdb_df_sd)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzgnAJfm_Os-"
      },
      "source": [
        "Looking back at what data is missing, we can see that not every movie has a duration available.  If we want to fill in these missing duration values, a crude way of doing this would be to replace the missing values with the mean movie duration.  Since most movies have durations that fall within a range of times, this would be better than simply setting the missing values to zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVm5muwU_Os_"
      },
      "source": [
        "imdb_df.duration = imdb_df.duration.fillna(imdb_df.duration.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzIzGk0Q_OtA"
      },
      "source": [
        "missingCount = imdb_df.isnull().sum()\n",
        "missingCount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZCxbJ1X_OtC"
      },
      "source": [
        "# Replace the missing values in the remaining columns. \n",
        "# Re-calculate missingCount to show that you have successfully replaced them.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMvXOkiF_OtJ"
      },
      "source": [
        "imdb_df.num_critic_for_reviews = imdb_df.num_critic_for_reviews.fillna(imdb_df.num_critic_for_reviews.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lp7xUoX4ymM"
      },
      "source": [
        "imdb_df.actor_3_facebook_likes = imdb_df.actor_3_facebook_likes.fillna(imdb_df.actor_3_facebook_likes.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5N_wXIP48UZ"
      },
      "source": [
        "imdb_df.actor_1_facebook_likes = imdb_df.actor_1_facebook_likes.fillna(imdb_df.actor_1_facebook_likes.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WackxZw45Hyu"
      },
      "source": [
        "imdb_df.actor_2_facebook_likes = imdb_df.actor_2_facebook_likes.fillna(imdb_df.actor_2_facebook_likes.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K0oUrWr5K98"
      },
      "source": [
        "imdb_df.gross = imdb_df.gross.fillna(imdb_df.gross.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRyc2-eT5UQk"
      },
      "source": [
        "imdb_df.facenumber_in_poster = imdb_df.facenumber_in_poster.fillna(imdb_df.facenumber_in_poster.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwmiDnHV5cuX"
      },
      "source": [
        "imdb_df.num_user_for_reviews = imdb_df.num_user_for_reviews.fillna(imdb_df.num_user_for_reviews.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQLlaq2R5nqg"
      },
      "source": [
        "imdb_df.budget = imdb_df.budget.fillna(imdb_df.budget.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvPR-YeK5xxK"
      },
      "source": [
        "imdb_df.aspect_ratio = imdb_df.aspect_ratio.fillna(imdb_df.aspect_ratio.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQkGyRzA570W"
      },
      "source": [
        "missingCount = imdb_df.isnull().sum()\n",
        "missingCount"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}